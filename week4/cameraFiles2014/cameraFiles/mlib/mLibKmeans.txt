import org.apache.spark.mllib.clustering.KMeans 
// Load and parse the data
 val data = sc.textFile("kmeans_data.txt")
 val parsedData = data.map( _.split(' ').map(_.toDouble))
 // Cluster the data into two classes using KMeans 
val numIterations = 20 
val numClusters = 2 
val clusters = KMeans.train(parsedData, numClusters, numIterations)
 // Evaluate clustering by computing Within Set Sum of Squared Errors 
val WSSSE = clusters.computeCost(parsedData) 
println("Within Set Sum of Squared Errors = " + WSSSE)
